{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bfc3fa48-fd57-4aee-95e7-954c311a2841",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfc3fa48-fd57-4aee-95e7-954c311a2841",
        "outputId": "b217d675-2a6e-4b44-da6e-1161f58a7f80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.41.2.post2-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Collecting peft\n",
            "  Downloading peft-0.6.2-py3-none-any.whl (174 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.7/174.7 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl\n",
            "  Downloading trl-0.7.4-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.9/133.9 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Collecting tyro>=0.5.11 (from trl)\n",
            "  Downloading tyro-0.5.18-py3-none-any.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl)\n",
            "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.7.0)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n",
            "  Downloading shtab-1.6.4-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
            "Installing collected packages: bitsandbytes, shtab, pyarrow-hotfix, docstring-parser, dill, multiprocess, tyro, accelerate, datasets, trl, peft\n",
            "Successfully installed accelerate-0.24.1 bitsandbytes-0.41.2.post2 datasets-2.15.0 dill-0.3.7 docstring-parser-0.15 multiprocess-0.70.15 peft-0.6.2 pyarrow-hotfix-0.6 shtab-1.6.4 trl-0.7.4 tyro-0.5.18\n"
          ]
        }
      ],
      "source": [
        "!pip install torch accelerate bitsandbytes datasets transformers peft trl scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "59539668-35eb-4813-a0a7-c88b00047a34",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59539668-35eb-4813-a0a7-c88b00047a34",
        "outputId": "67b204a4-b7c8-4e94-bb00-47a65adc1870"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Nov 26 22:01:04 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "46c75cb1-f815-41b0-b001-1b3920ff1bf2",
      "metadata": {
        "id": "46c75cb1-f815-41b0-b001-1b3920ff1bf2"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import bitsandbytes as bnb\n",
        "from datasets import load_dataset\n",
        "from functools import partial\n",
        "import os\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
        "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bb9fd97-f285-471d-82e8-ebc732ef2393",
      "metadata": {
        "id": "3bb9fd97-f285-471d-82e8-ebc732ef2393"
      },
      "source": [
        "***Get Model***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa647af6-fd30-46e7-9eeb-eaeb808c379d",
      "metadata": {
        "id": "aa647af6-fd30-46e7-9eeb-eaeb808c379d"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "179326dd-3f03-48d7-aa80-38e782f84486",
      "metadata": {
        "id": "179326dd-3f03-48d7-aa80-38e782f84486"
      },
      "outputs": [],
      "source": [
        "#model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
        "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "665f95b2-bda1-452f-8266-f4d5f1eac49d",
      "metadata": {
        "id": "665f95b2-bda1-452f-8266-f4d5f1eac49d"
      },
      "outputs": [],
      "source": [
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "# begin initializing HF items, need auth token for these\n",
        "\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10f277b5-8d5a-48cc-afce-6020d707b347",
      "metadata": {
        "id": "10f277b5-8d5a-48cc-afce-6020d707b347"
      },
      "outputs": [],
      "source": [
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    use_auth_token=True\n",
        ")\n",
        "model.eval()\n",
        "print(f\"Model loaded on {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0319a24-5a80-41d6-bb2a-c6052f307621",
      "metadata": {
        "id": "b0319a24-5a80-41d6-bb2a-c6052f307621"
      },
      "outputs": [],
      "source": [
        "mem = model.get_memory_footprint()\n",
        "print(\"Memory footprint: {} \".format(mem))\n",
        "\n",
        "# should be (7B) 7,000,000,000*4(Int4) / 8(8 bits per byte) = 3,500,000,000 = 3.5GB\n",
        "# actual (7B)  3,829,940,224 (not all weights become int 4)\n",
        "# actual (13B)  7,083,970,560  (not all weights become int 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "596a618a-bc04-4b06-8402-adbc49ae70dd",
      "metadata": {
        "id": "596a618a-bc04-4b06-8402-adbc49ae70dd"
      },
      "outputs": [],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "371bf735-6bb8-4bf4-81d3-26b0cb563219",
      "metadata": {
        "id": "371bf735-6bb8-4bf4-81d3-26b0cb563219"
      },
      "source": [
        "***Get Dataset***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14cb2fa9-2189-4833-9382-f0bf7f9f1374",
      "metadata": {
        "id": "14cb2fa9-2189-4833-9382-f0bf7f9f1374"
      },
      "outputs": [],
      "source": [
        "# Load the dataset from Hugging Face\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"kaist-ai/CoT-Collection\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "713ed372-c7c1-4ba3-a578-1000effbbbb7",
      "metadata": {
        "id": "713ed372-c7c1-4ba3-a578-1000effbbbb7"
      },
      "outputs": [],
      "source": [
        "print(f'Number of records: {len(dataset)}')\n",
        "print(f'Column names are: {dataset.column_names}')\n",
        "\n",
        "'''\n",
        "Number of records: 1837928\n",
        "Column names are: ['source', 'target', 'rationale', 'task', 'type']\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e84dca76-0e32-43d5-9957-2ab8ac1f8099",
      "metadata": {
        "id": "e84dca76-0e32-43d5-9957-2ab8ac1f8099"
      },
      "outputs": [],
      "source": [
        "#all are CoT but being sure\n",
        "dataset_cot = dataset.filter(lambda example: example['type'] == \"CoT\")\n",
        "print(f'Number of records: {len(dataset_cot)}')\n",
        "print(f'Column names are: {dataset_cot.column_names}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aba2228b-49a8-408b-a0a4-1da0a8586913",
      "metadata": {
        "id": "aba2228b-49a8-408b-a0a4-1da0a8586913"
      },
      "outputs": [],
      "source": [
        "def create_prompt(rec):\n",
        "\n",
        "  start = \"Read the Instruction below and provide an answer.\"\n",
        "  question = f\"### INSTRUCTION:\\n{rec['source']}\\n\\n\"\n",
        "  response = f\"### RESPONSE:\\n{rec['rationale']}\\n\"\n",
        "  answer = f\"Therefore the answer is {rec['target']}\\n\\n\"\n",
        "  end = \"### End\"\n",
        "\n",
        "  parts = [part for part in [start, question, response, answer, end] if part]\n",
        "\n",
        "  formatted_prompt = \"\\n\\n\".join(parts)\n",
        "  formatted_prompt = formatted_prompt.replace('\\\\n', '\\n')\n",
        "\n",
        "  rec[\"text\"] = formatted_prompt\n",
        "\n",
        "  return rec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86cc9de1-a07d-462f-99bb-d19375a2f604",
      "metadata": {
        "id": "86cc9de1-a07d-462f-99bb-d19375a2f604"
      },
      "outputs": [],
      "source": [
        "p = create_prompt(dataset_cot[30000])\n",
        "print(p)\n",
        "print(p[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "479dcd97-6e1c-4cc5-870b-740325c8d1bc",
      "metadata": {
        "id": "479dcd97-6e1c-4cc5-870b-740325c8d1bc"
      },
      "outputs": [],
      "source": [
        "dataset = dataset_cot.map(create_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M6mqZqmOmx0C",
      "metadata": {
        "id": "M6mqZqmOmx0C"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.map(\n",
        "        batched=True,\n",
        "        remove_columns=['source', 'target', 'rationale', 'task', 'type']\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a747d5f2-416b-4233-ac16-b6caab85db05",
      "metadata": {
        "id": "a747d5f2-416b-4233-ac16-b6caab85db05"
      },
      "outputs": [],
      "source": [
        "print(dataset[30000][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c33d35c-ba3f-46e4-aea7-a778c1b07da8",
      "metadata": {
        "id": "5c33d35c-ba3f-46e4-aea7-a778c1b07da8"
      },
      "outputs": [],
      "source": [
        "#Save dataset to the hub for future use\n",
        "#dataset.push_to_hub(\"Venkat-Ram-Rao/processed_cot_dataset\", private=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DqXsa39NnS6j",
      "metadata": {
        "id": "DqXsa39NnS6j"
      },
      "outputs": [],
      "source": [
        "#max length of the model\n",
        "def get_max_length(model):\n",
        "    conf = model.config\n",
        "    max_length = None\n",
        "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
        "        max_length = getattr(model.config, length_setting, None)\n",
        "        if max_length:\n",
        "            print(f\"Found max lenth: {max_length}\")\n",
        "            break\n",
        "    if not max_length:\n",
        "        max_length = 1024\n",
        "        print(f\"Using default max length: {max_length}\")\n",
        "    return max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_09bRyoCnVJS",
      "metadata": {
        "id": "_09bRyoCnVJS"
      },
      "outputs": [],
      "source": [
        "mx = get_max_length(model)\n",
        "mx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UZgUZtsKmZN8",
      "metadata": {
        "id": "UZgUZtsKmZN8"
      },
      "outputs": [],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5P39bQBYnqZD",
      "metadata": {
        "id": "5P39bQBYnqZD"
      },
      "outputs": [],
      "source": [
        "#tokenize dataset\n",
        "dataset = dataset.map(lambda samples: tokenizer(samples['text']), batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L-a4MiG_oKpl",
      "metadata": {
        "id": "L-a4MiG_oKpl"
      },
      "outputs": [],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56077f06-5948-4633-9f52-87c65e6108f1",
      "metadata": {
        "id": "56077f06-5948-4633-9f52-87c65e6108f1"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < mx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7Ie8I9UMoPYr",
      "metadata": {
        "id": "7Ie8I9UMoPYr"
      },
      "outputs": [],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99277f0a-85f2-4ae3-940b-21c0709b0ce1",
      "metadata": {
        "id": "99277f0a-85f2-4ae3-940b-21c0709b0ce1"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "set_seed(seed)\n",
        "\n",
        "dataset = dataset.shuffle(seed=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "464fb551-91cb-4ed7-aa98-6c9b73543215",
      "metadata": {
        "id": "464fb551-91cb-4ed7-aa98-6c9b73543215"
      },
      "source": [
        "***Freeze Original Weights***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4f3d967-7133-4951-aa09-775d0278ff48",
      "metadata": {
        "id": "f4f3d967-7133-4951-aa09-775d0278ff48"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False  # freeze the model - train adapters later\n",
        "  if param.ndim == 1:\n",
        "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "    param.data = param.data.to(torch.float32)\n",
        "\n",
        "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "class CastOutputToFloat(nn.Sequential):\n",
        "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "model.lm_head = CastOutputToFloat(model.lm_head)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da1cce0b-c693-42e2-a15c-f9211159bcd3",
      "metadata": {
        "id": "da1cce0b-c693-42e2-a15c-f9211159bcd3"
      },
      "source": [
        "***Create Lora Config***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dd453ee-1ed9-4705-abdb-4d0165e30863",
      "metadata": {
        "id": "4dd453ee-1ed9-4705-abdb-4d0165e30863"
      },
      "outputs": [],
      "source": [
        "def find_all_linear_names(model):\n",
        "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, cls):\n",
        "            names = name.split('.')\n",
        "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "\n",
        "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
        "        lora_module_names.remove('lm_head')\n",
        "    return list(lora_module_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7be69561-ed0a-438f-801c-a086f0e9ec56",
      "metadata": {
        "id": "7be69561-ed0a-438f-801c-a086f0e9ec56"
      },
      "outputs": [],
      "source": [
        "modules = find_all_linear_names(model)\n",
        "print(modules)\n",
        "\n",
        "#['v_proj', 'up_proj', 'down_proj', 'k_proj', 'o_proj', 'q_proj', 'gate_proj']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95b2e91e-bdaf-48ab-98a2-c9b9808f8586",
      "metadata": {
        "id": "95b2e91e-bdaf-48ab-98a2-c9b9808f8586"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,  #attention heads\n",
        "    lora_alpha=64,  #alpha scaling\n",
        "    target_modules=modules,  #gonna train all\n",
        "    lora_dropout=0.1,  # dropout probability for layers\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\", #for Decoder models like GPT Seq2Seq for Encoder-Decoder models like T5\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd7484de-88e7-43b4-90e8-547b0cd0038f",
      "metadata": {
        "id": "cd7484de-88e7-43b4-90e8-547b0cd0038f"
      },
      "outputs": [],
      "source": [
        "##Get the PEFT Model using the downloaded model and the loRA config\n",
        "model = get_peft_model(model, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98161e8b-41de-47ed-826b-0edd7bbac064",
      "metadata": {
        "id": "98161e8b-41de-47ed-826b-0edd7bbac064"
      },
      "source": [
        "***Training***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55272b09-29df-493a-ba4f-a638fb2097a5",
      "metadata": {
        "id": "55272b09-29df-493a-ba4f-a638fb2097a5"
      },
      "outputs": [],
      "source": [
        "# Print Trainable parameters\n",
        "trainable_params = 0\n",
        "all_param = 0\n",
        "for _, param in model.named_parameters():\n",
        "    all_param += param.numel()\n",
        "    if param.requires_grad:\n",
        "        trainable_params += param.numel()\n",
        "print(\n",
        "    f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9a550e9-0518-4e43-8b5e-db1b0ff58c68",
      "metadata": {
        "id": "c9a550e9-0518-4e43-8b5e-db1b0ff58c68"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "666f941a-5efb-4f84-96b9-c86a33ae3962",
      "metadata": {
        "id": "666f941a-5efb-4f84-96b9-c86a33ae3962"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=10,\n",
        "        max_steps=100, #20,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "    ),\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "\n",
        "model.config.use_cache = False  # re-enable for inference to speed up predictions for similar inputs\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef517c2c-382d-4eb4-8539-45f44a48d49d",
      "metadata": {
        "id": "ef517c2c-382d-4eb4-8539-45f44a48d49d"
      },
      "source": [
        "***Push to Hub***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8446eed7-ea82-47b5-814f-606055667c37",
      "metadata": {
        "id": "8446eed7-ea82-47b5-814f-606055667c37"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub(\"Venkat-Ram-Rao/Llama2_7B_qlora_CoT_FT-v2\",\n",
        "                  use_auth_token=True,\n",
        "                  commit_message=\"fine tuned on kaist-ai/CoT-Collection\",\n",
        "                  private=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a55e523d-4bf5-4f91-88dd-ac79c6f55986",
      "metadata": {
        "id": "a55e523d-4bf5-4f91-88dd-ac79c6f55986"
      },
      "source": [
        "***Load from Hub***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1dccca5-1535-46b3-9716-41862d0aa5a5",
      "metadata": {
        "id": "a1dccca5-1535-46b3-9716-41862d0aa5a5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "peft_model_id = \"Venkat-Ram-Rao/Llama2_7B_qlora_CoT_FT\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=True, device_map='auto')\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RieOla-kqpwv",
      "metadata": {
        "id": "RieOla-kqpwv"
      },
      "outputs": [],
      "source": [
        "mem = model.get_memory_footprint()\n",
        "print(\"Memory footprint: {} \".format(mem))\n",
        "\n",
        "#7,227,846,656"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6ac3697-c284-49c8-bcb5-8932f9154671",
      "metadata": {
        "id": "a6ac3697-c284-49c8-bcb5-8932f9154671"
      },
      "source": [
        "***Inference***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd2546b3-c17f-410a-ba19-8d14d3953aee",
      "metadata": {
        "id": "bd2546b3-c17f-410a-ba19-8d14d3953aee"
      },
      "outputs": [],
      "source": [
        "tst = \"\"\"Read the Instruction below and provide an answer.\n",
        "\n",
        "### INSTRUCTION:\n",
        "In this task, you are given an input list A. You need to find all the elements of the list that are numbers and calculate their sum.\n",
        "\n",
        "['i', 'P', 'h', '849', 'e']\n",
        "\n",
        "\n",
        "\n",
        "### RESPONSE:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fce3b926-bc65-472e-8f77-05a42cee3e08",
      "metadata": {
        "id": "fce3b926-bc65-472e-8f77-05a42cee3e08"
      },
      "outputs": [],
      "source": [
        "batch = tokenizer(tst, return_tensors='pt')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m6adldKVq2Uw",
      "metadata": {
        "id": "m6adldKVq2Uw"
      },
      "outputs": [],
      "source": [
        "with torch.cuda.amp.autocast():\n",
        "  output_tokens = model.generate(**batch, max_new_tokens=90)\n",
        "\n",
        "print('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))\n",
        "\n",
        "'''\n",
        "EXPECTED ANSWER:\n",
        "\n",
        "### RESPONSE:\n",
        "The given list ['i', 'P', 'h', '849', 'e'] contains the number 849, which is the only element of the list that is a number. Therefore, the final answer is 849.\n",
        "\n",
        "Therefore the answer is 849\n",
        "\n",
        "### End'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M2buO_Z2Yaar",
      "metadata": {
        "id": "M2buO_Z2Yaar"
      },
      "source": [
        "Test2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x8Wiw3M2YcB9",
      "metadata": {
        "id": "x8Wiw3M2YcB9"
      },
      "outputs": [],
      "source": [
        "tst = \"\"\"Read the Instruction below and provide an answer.\n",
        "\n",
        "### INSTRUCTION:\n",
        "Sam is taller than Bob. Jack is taller than Sam. Is Jack taller than Bob?\n",
        "Answer Yes or No\n",
        "\n",
        "\n",
        "\n",
        "### RESPONSE:\"\"\"\n",
        "\n",
        "batch = tokenizer(tst, return_tensors='pt')\n",
        "\n",
        "with torch.cuda.amp.autocast():\n",
        "  output_tokens = model.generate(**batch, max_new_tokens=90)\n",
        "\n",
        "print('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WOJzxkKAZsV-",
      "metadata": {
        "id": "WOJzxkKAZsV-"
      },
      "outputs": [],
      "source": [
        "tst = \"\"\"Read the Instruction below and provide an answer.\n",
        "\n",
        "### INSTRUCTION:\n",
        "Sam is taller than Bob. Jack is taller than Sam.\n",
        "Who is the tallest? Bob, Sam or Jack?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### RESPONSE:\"\"\"\n",
        "\n",
        "batch = tokenizer(tst, return_tensors='pt')\n",
        "\n",
        "with torch.cuda.amp.autocast():\n",
        "  output_tokens = model.generate(**batch, max_new_tokens=90)\n",
        "\n",
        "print('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IxV10nyhaeZQ",
      "metadata": {
        "id": "IxV10nyhaeZQ"
      },
      "outputs": [],
      "source": [
        "tst = \"\"\"Read the Instruction below and provide an answer.\n",
        "\n",
        "### INSTRUCTION:\n",
        "In this task, you are given an input list A. You need to find all the elements of the list that are numbers and calculate their sum.\n",
        "\n",
        "['i', '100', 'h', '849', 'e']\n",
        "\n",
        "\n",
        "\n",
        "### RESPONSE:\"\"\"\n",
        "\n",
        "batch = tokenizer(tst, return_tensors='pt')\n",
        "\n",
        "with torch.cuda.amp.autocast():\n",
        "  output_tokens = model.generate(**batch, max_new_tokens=90)\n",
        "\n",
        "print('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aFGadjcUoUMW",
      "metadata": {
        "id": "aFGadjcUoUMW"
      },
      "outputs": [],
      "source": [
        "tst = \"\"\"Read the Instruction below and provide an answer.\n",
        "\n",
        "### INSTRUCTION:\n",
        "In this task, you are given an input list A. You need to find all the elements of the list that are numbers and calculate their sum.\n",
        "\n",
        "['i', '100', 'h', '849', '100']\n",
        "\n",
        "\n",
        "\n",
        "### RESPONSE:\"\"\"\n",
        "\n",
        "batch = tokenizer(tst, return_tensors='pt')\n",
        "\n",
        "with torch.cuda.amp.autocast():\n",
        "  output_tokens = model.generate(**batch, max_new_tokens=90)\n",
        "\n",
        "print('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y82ZPeWYoWme",
      "metadata": {
        "id": "Y82ZPeWYoWme"
      },
      "outputs": [],
      "source": [
        "tst = \"\"\"Read the Instruction below and provide an answer.\n",
        "\n",
        "### INSTRUCTION:\n",
        "In this task, you need to provide the correct option for a given problem on probability from the provided options. Problem: there is a 50 % chance jen will visit chile this year , while there is a 50 % chance that she will visit madagascar this year . what is the probability that jen will visit either chile or madagascar this year , but not both ?\n",
        "Options: a ) 25.0 % , b ) 50.0 % , c ) 62.5 % , d ) 75.0 % , e ) 80.0 %\n",
        "\n",
        "\n",
        "\n",
        "### RESPONSE:\"\"\"\n",
        "\n",
        "batch = tokenizer(tst, return_tensors='pt')\n",
        "\n",
        "with torch.cuda.amp.autocast():\n",
        "  output_tokens = model.generate(**batch, max_new_tokens=90)\n",
        "\n",
        "print('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oSBfVoOzoo24",
      "metadata": {
        "id": "oSBfVoOzoo24"
      },
      "outputs": [],
      "source": [
        "tst = \"\"\"Read the Instruction below and provide an answer.\n",
        "\n",
        "### INSTRUCTION:\n",
        "In this task you will be given a list of numbers and you need to find the mean (average) of that list. The mean of a list can be found by summing every number in the list then dividing the result by the size of that list. The output should be rounded to 3 decimal places.\n",
        "\n",
        " [-43.959, 161.939]\n",
        "\n",
        "\n",
        "\n",
        "### RESPONSE:\"\"\"\n",
        "\n",
        "batch = tokenizer(tst, return_tensors='pt')\n",
        "\n",
        "with torch.cuda.amp.autocast():\n",
        "  output_tokens = model.generate(**batch, max_new_tokens=90)\n",
        "\n",
        "print('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}